# -*- coding: utf-8 -*-
"""Housing Workshop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GB5iYyipyd5y-xMOqoN0yFkwNjs9q7eA

# Housing Price Prediction
Creator: Mike Zhang, Gavin Firestone, Eamonn Keane, Ioan-Alexandru Mirica

## Read in Data
Import needed packages
"""

# Import packages

import pandas as pd
import numpy as np
from google.colab import drive
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import root_mean_squared_error
from sklearn.metrics import r2_score

#read the file
df = pd.read_csv('train.csv')
df.head()

"""## Data Pre-pocessing

Investigate:
- the given attributes
- data types
- identify non-numerical data difficult to encode
- missing data
- the attribues with the highest correlation to sales price
- statistical summary operations on some data collections (mean, median, count, standard devation, groupbys)
"""

df.shape

df.dtypes

"""For the attribute that is not numerical, check if it is valuabe to do the encoding


"""

df_non_numerical = df.select_dtypes(exclude = np.number)
df_non_numerical.nunique()

df_non_numerical.loc[:, df.select_dtypes(exclude = np.number).nunique() > 5].nunique()

"""Check missing data"""

df.isna().sum().sort_values(ascending = False).head(20)

"""Check the numerical data that has a lot of "0"s"""

numerical_df = df.select_dtypes(include = np.number)
(numerical_df == 0).sum().sort_values(ascending = False).head(20)

numerical_df.isna().sum().sort_values(ascending = False).head()

"""Get the numerical attributes with the highest correlation to the sales **price**"""

numerical_df.corr(method='pearson').loc[:, ['SalePrice']].sort_values(by = ['SalePrice'], ascending = False)

print(df['SalePrice'].median())
print(df['SalePrice'].std())

df[df["Utilities"] == "AllPub"].shape

# price by neighborhood standard deviation
std_price_by_neighborhood = df[["SalePrice", "Neighborhood"]].groupby("Neighborhood").std()
std_price_by_neighborhood.sort_values(by="SalePrice", ascending=False)

# average price by neighborhood
avg_price_by_neighborhood = df[["SalePrice", "Neighborhood"]].groupby("Neighborhood").mean()
avg_price_by_neighborhood.sort_values(by="SalePrice", ascending=False)

# overall quality by neighborhood
price_by = df[["OverallQual", "Neighborhood"]].groupby("Neighborhood").mean()
price_by.sort_values(by="OverallQual", ascending=False)

"""## Data Processing

### Data Cleaning
- Cleaning the attribute with a lot of meaningless n/a data

- Replace meaningful n/a data with numerical value
"""

# drop houses with n/a for electrical system since there is only one house
df.dropna(subset=["Electrical"], inplace=True)
print(df.shape)

# drop houses without all public utilities since there is only one house
# then drop the utilities column
df = df.drop(df[df["Utilities"] != "AllPub"].index)
print(df.shape)
df = df.drop(['Utilities'], axis=1)
print(df.shape)

# drop houses with pools
df = df.drop(df[df["PoolArea"] != 0].index)
print(df.shape)
df = df.drop(['PoolArea', 'PoolQC'], axis=1)
print(df.shape)

# if Masonry veneer type n/a, set to 0
df["MasVnrArea"] = df["MasVnrArea"].fillna(0)
print(df.shape)

# set the garage year to the default value when the house doesn't have a garage.
df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['GarageYrBlt'].mean())
print(df['GarageYrBlt'].isna().sum())

# if lotfrontage n/a, set to 0 (check if this is more accurate than dropping column)
df['LotFrontage'] = df['LotFrontage'].fillna(0)
print(df.shape)
df.isna().sum().sort_values(ascending = False).head()

# change the garage_area garage_cars
df['GarageArea'] = df['GarageArea'].fillna(0)
print(df.shape)
df['GarageCars'] = df['GarageCars'].fillna(0)
print(df.shape)
df.isna().sum().sort_values(ascending = False).head(20)

"""### Data Transformation

- Encoding

- Standardization
"""

# if bsmtqual n/a, set to 0 (will be label encoded)
df['BsmtQual'] = df['BsmtQual'].fillna(0)
# one-hot encode foundation, centralair
df = pd.get_dummies(df, columns=['Foundation', 'CentralAir'], drop_first=True, dtype=int)
df.head()
# label encode exterior quality, kitchen quality, basement quality
from sklearn.preprocessing import LabelEncoder
label_mapping = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
df = df.replace({'ExterQual': label_mapping})
df = df.replace({'KitchenQual': label_mapping})
df = df.replace({'BsmtQual': label_mapping})
df["KitchenQual"].isna().sum()
df["ExterQual"].isna().sum()
print(df["BsmtQual"].isna().sum())

numerical_df = df.select_dtypes(include=np.number) #get only numerical columns
numerical_df.corr()["SalePrice"].sort_values(ascending=False)





"""Notes:

## Data Visualization

### Boxplot

Boxplot for Sale Price Distribution
"""

plt.figure(figsize=(3,7))
df["SalePrice"].plot(kind="box")
plt.title("SalePrice Distribution")
plt.ylabel("SalePrice")
plt.show()

# sorted by the median price by neighborhood
median_price_by_neighborhood = df[["SalePrice", "Neighborhood"]].groupby("Neighborhood").median()
sorted_price = median_price_by_neighborhood.sort_values(by="SalePrice", ascending=False)

#make a boxplot
plt.figure(figsize=(7, 7))
sns.boxplot(data=df, x="SalePrice", y="Neighborhood", order=sorted_price.index)
plt.show()

"""### Histogram

SalePrice Distribution Diagram
"""

plt.figure(figsize=(7, 7))
df["SalePrice"].hist(bins=100)
plt.xlabel("SalePrice")
plt.ylabel("Count")
plt.title("SalePrice Distribution")
plt.show()
print("Standard Deviation:" + str(df["SalePrice"].std()))
print("Mean:" + str(df["SalePrice"].mean()))
print("Median:" + str(df["SalePrice"].median()))
print("Min:" + str(df["SalePrice"].min()))
print("Max:" + str(df["SalePrice"].max()))

"""Year Built Histogram"""

plt.figure(figsize=(7, 7))
df["YearBuilt"].hist(bins=100)
plt.xlabel("YearBuilt")
plt.ylabel("Count")
plt.title("Year Built Distribution")
plt.show()

df_num = df.select_dtypes(include = ['float64', 'int64'])
df_num.hist(figsize=(16, 20), bins=100, xlabelsize=8, ylabelsize=8)

"""### Line Plot

"""

plt.figure(figsize=(7,5))
median_price_df = df[['YearBuilt', 'SalePrice']].groupby(['YearBuilt']).median().reset_index()
plt.scatter('YearBuilt', 'SalePrice', data=df, s=0.1)
plt.plot('YearBuilt', 'SalePrice', data=median_price_df)
plt.xlabel("YearBuilt")
plt.ylabel("SalePrice")
plt.title("Sale Price vs YearBuilt")
plt.legend(['Sale Price of a house','median Sale Prices of houses by YearBuilt'], loc = 2)
#generate similar histogram for other data

plt.figure(figsize=(7,5))
median_price_df = df[['OverallQual', 'SalePrice']].groupby(['OverallQual']).median().reset_index()
plt.scatter('OverallQual', 'SalePrice', data=df, s=0.1)
plt.plot('OverallQual', 'SalePrice', data=median_price_df)
plt.xlabel("Overall Quality of the House")
plt.ylabel("SalePrice")
plt.title("Sale Price vs Overall Quality")
plt.legend(['Sale Price of a house','median Sale Prices of houses by Overall Quality'], loc = 2)
#generate similar histogram for other data



"""Heat Map"""

sns.heatmap(numerical_df.corr(method='pearson'))

"""## Data Modeling

### Linear Regression
"""

numerical_df = df.select_dtypes(include = np.number)
X = numerical_df.drop(['SalePrice', 'Id'],  axis=1)
y = df[['SalePrice']].values.ravel()

print(f"X: {X.shape}   |    y: {y.shape}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

print(f"Test:  X: {X_test.shape}    |    y: {y_test.shape}")
print(f"Train: X: {X_train.shape}   |    y: {y_train.shape}")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled

lin_reg_model = LinearRegression()
lin_reg_model.fit(X_train_scaled, y_train)
y_pred = lin_reg_model.predict(X_test_scaled)

print(y_pred.shape)
print(y_test.shape)

df.isna().sum().sort_values(ascending = False).head(20)

rmse = root_mean_squared_error(y_test, y_pred)
print(f'RMSE: {rmse}')
r2 = r2_score(y_test, y_pred)
print(f'R^2: {r2}')
mae = mean_absolute_error(y_test, y_pred)
print(f'MAE: {round(mae,5)}')
linreg_coef = pd.DataFrame({"weight": lin_reg_model.coef_})
linreg_coef["coef"] = X.columns
linreg_coef = linreg_coef[["coef", "weight"]]
linreg_coef.sort_values(by="weight", ascending=False)

# visualize the prediction error
diagram = y_test - y_pred
plt.scatter(y_pred, diagram)
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs. Predicted Values (Linear Regression)')
plt.axhline(y=0, color='red', linestyle='-')
plt.show()

plt.figure(figsize=(7, 7))
histogram = pd.DataFrame({'data': diagram})
histogram['data'].hist(bins=75)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Plot (Linear Regression)')
plt.show()

"""### Ridge Model (L2)"""

from sklearn.linear_model import Ridge

ridge_model = Ridge(alpha=100)
ridge_model.fit(X_train_scaled, y_train)
y_pred = ridge_model.predict(X_test_scaled)

from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error
rmse = root_mean_squared_error(y_test, y_pred)
print(f'RMSE: {round(rmse, 5)}')
r2 = r2_score(y_test, y_pred)
print(f'R^2: {round(r2,5)}')
mae = mean_absolute_error(y_test, y_pred)
print(f'MAE: {round(mae,5)}')





"""### Lasso Model (L1)

"""

from sklearn.linear_model import Lasso

lasso_model = Lasso(alpha=100)
lasso_model.fit(X_train_scaled, y_train)
y_pred = lasso_model.predict(X_test_scaled)

from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error
rmse = root_mean_squared_error(y_test, y_pred)
print(f'RMSE: {round(rmse, 5)}')
r2 = r2_score(y_test, y_pred)
print(f'R^2: {round(r2,5)}')
mae = mean_absolute_error(y_test, y_pred)
print(f'MAE: {round(mae,5)}')

"""###Neural Network

Preparing data for neural network
"""

# Take numeric variables only and cast to float32
df_numeric = df.select_dtypes(include='number')
df_numeric = df_numeric.astype('float32')

# Create explanatory and dependent sets
target = ['SalePrice']
y = df_numeric[target].values.ravel()
X = df_numeric.drop(target+['Id'], axis=1)
print(y.shape)
print(X.shape)

# Create training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
print(f"Train: X: {X_train.shape}     |    y: {y_train.shape}")
print(f"Test:  X: {X_test.shape}     |    y: {y_test.shape}")

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()

"""Creating and training the neural network"""

# Declare neural network
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with 256 nodes
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1)  # Single output node for regression
])

# Compile the model
odel.compile(optimizer=tf.keras.optimizers.Adam(0.001),
              loss='mean_squared_error',
              metrics=['mae'])

# Train the model
history = model.fit(X_train_scaled, y_train_scaled,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    verbose=1)

"""Post-processing and analysis"""

# Scale data back to original range and print accuracy metrics
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

predictions_scaled = model.predict(X_test_scaled)
predictions = scaler_y.inverse_transform(predictions_scaled)

print(max(predictions))
print(max(y_test))
print(min(predictions))
print(min(y_test))
print(sum(predictions)/len(predictions))
print(sum(y_test)/len(y_test))

sumsqe = 0
for i in range(len(predictions)):
  sumsqe += (y_test[i] - predictions[i])
avgsqe = sumsqe / len(predictions)
print(avgsqe)

print(r2_score(y_test, predictions))

# Verify weight matrices
weights = model.get_weights()
print(weights)

# Plot predicted vs actual prices
plt.scatter(y_test, predictions)
plt.xlim(0,500000)
plt.ylim(0,500000)
plt.title('Predicted vs Actual Sale Prices')
plt.xlabel('Actual Sale Price')
plt.ylabel('Predicted Sale Price')

# Plot residuals vs predictions
residuals = []
for i in range(len(y_test)):
  residuals.append(y_test[i] - predictions[i])
plt.scatter(predictions, residuals)
plt.xlim(0,500000)
plt.ylim(-150000,150000)
plt.title('Residuals vs Predicted Values')
plt.xlabel('Predicted Sale Price')
plt.ylabel('Residuals')

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=1750, max_depth=16)
rf_model.fit(X_train_scaled, y_train)
y_pred = rf_model.predict(X_test_scaled)

from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error
rmse = root_mean_squared_error(y_test, y_pred)
print(f'RMSE: {round(rmse, 5)}')
r2 = r2_score(y_test, y_pred)
print(f'R^2: {round(r2,5)}')
mae = mean_absolute_error(y_test, y_pred)
print(f'MAE: {round(mae,5)}')