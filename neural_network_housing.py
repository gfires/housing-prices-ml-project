# -*- coding: utf-8 -*-
"""Neural_Network_Housing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1avB42O-iyGOJ1T93mULRiuKqJhzyrEu3

# Deep Learning with Tensorflow

The goal of this workshop is to understand how to develop neural networks to identify a handwritten digit in the MNIST database.

## Content

1. Import data
2. Clean data
  * Scaling image data
  * One-hot encoding classification labels
3. Build a fully connected model
4. Fit model to training data
5. Generate modelâ€™s classifications on test data
6. Compute model accuracy
7. Implement the ML model pipeline for a convolutional neural network

# Section 0: Import Libraries & Dataset

Import necessary libraries and training data

---
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
df = pd.read_csv('train.csv')

from sklearn.metrics import r2_score

"""Clean data"""

# drop houses without all public utilities since there is only one house
# then drop the utilities column
# print(df.shape)
# numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
# df = df.select_dtypes(include=numerics)
# print(df.shape)
# print(df.isna().sum())
# df.drop(columns=['LotFrontage', 'GarageYrBlt', 'MasVnrArea'], inplace=True)
# print(df.isna().sum())

# drop houses with n/a for electrical system since there is only one house
df.dropna(subset=["Electrical"], inplace=True)

df = df.drop(df[df["Utilities"] != "AllPub"].index)
df = df.drop(['Utilities'], axis=1)

# drop houses with pools
df = df.drop(df[df["PoolArea"] != 0].index)
df = df.drop(['PoolArea', 'PoolQC'], axis=1)

# if Masonry veneer type n/a, set to 0
df["MasVnrArea"] = df["MasVnrArea"].fillna(0)

# set the garage year to the default value when the house doesn't have a garage.
df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['GarageYrBlt'].mean())

# if lotfrontage n/a, set to 0 (check if this is more accurate than dropping column)
df['LotFrontage'] = df['LotFrontage'].fillna(0)
df.isna().sum().sort_values(ascending = False).head()

# change the garage_area garage_cars
df['GarageArea'] = df['GarageArea'].fillna(0)
print(df.shape)
df['GarageCars'] = df['GarageCars'].fillna(0)
print(df.shape)
df.isna().sum().sort_values(ascending = False).head(20)

print(df.shape)

"""Encoding"""

# if bsmtqual n/a, set to 0 (will be label encoded)
df['BsmtQual'] = df['BsmtQual'].fillna(0)
# one-hot encode foundation, centralair
df = pd.get_dummies(df, columns=['Foundation', 'CentralAir'], drop_first=True, dtype=int)
df.head()
# label encode exterior quality, kitchen quality, basement quality
from sklearn.preprocessing import LabelEncoder
label_mapping = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
df = df.replace({'ExterQual': label_mapping})
df = df.replace({'KitchenQual': label_mapping})
df = df.replace({'BsmtQual': label_mapping})
df["KitchenQual"].isna().sum()
df["ExterQual"].isna().sum()
print(df["BsmtQual"].isna().sum())

"""Perform train-test split

Make numeric-only dataframe
"""

df_numeric = df.select_dtypes(include='number')
df_numeric = df_numeric.astype('float32')
print(df_numeric.shape)
print(df_numeric.dtypes)

"""Establish explanatory and dependent variables"""

target = ['SalePrice']
y = df_numeric[target].values.ravel()
X = df_numeric.drop(target+['Id'], axis=1)
print(y.shape)
print(X.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
print(f"Train: X: {X_train.shape}     |    y: {y_train.shape}")
print(f"Test:  X: {X_test.shape}     |    y: {y_test.shape}")
#print(np.any(np.isnan(X_train)), np.any(np.isnan(y_train)))
#print(np.any(np.isinf(X_train)), np.any(np.isinf(y_train)))

"""Transform data via scaling"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()

"""### **Q0.3** Scale image data

Get maximum instance of each variable and cast all to float32
"""

max_val = max(np.max(X_train), np.max(X_test))
print(max_val)

"""Scale data to be 0-1"""

X_train = X_train.astype("float32") / max_val      # make sure you only run this once!
X_test = X_test.astype("float32") / max_val
print(np.max(X_train), np.max(X_test))

"""Check for correct range

# Section 1: Build a Fully Connected Model
Using ```tensorflow```

### **Q1.2** Build the sequential model
"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with 256 nodes
    tf.keras.layers.Dense(128, activation='relu'),  # Optional hidden layer
    tf.keras.layers.Dense(1)  # Single output node for regression
])

"""### **Q1.5** Compile the model with the Adam optimizer"""

model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
              loss='mean_squared_error',
              metrics=['mae'])

"""### **Q1.6** Train the model with a batch size of 32 over 5 epochs"""

history = model.fit(X_train_scaled, y_train_scaled,
                    epochs=50,  # You can adjust the number of epochs based on performance
                    batch_size=32,  # Typical batch size
                    validation_split=0.2,  # Use 20% of the training data for validation
                    verbose=1)  # Print training progress

"""### **Q1.7** Evaluate the model and report the accuracy"""

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

"""### **Q1.8** Generate the model's predictions and visualize a few"""

predictions_scaled = model.predict(X_test_scaled)
predictions = scaler_y.inverse_transform(predictions_scaled)
print(max(predictions))
print(max(y_test))
print(min(predictions))
print(min(y_test))
print(sum(predictions)/len(predictions))
print(sum(y_test)/len(y_test))

sumsqe = 0
for i in range(len(predictions)):
  sumsqe += (y_test[i] - predictions[i])
avgsqe = sumsqe / len(predictions)
print(avgsqe)

print(r2_score(y_test, predictions))

"""### **Q1.9** Verify the model's weight matrices"""

weights = model.get_weights()
print(weights)

plt.scatter(y_test, predictions)
plt.xlim(0,500000)
plt.ylim(0,500000)
plt.title('Predicted vs Actual Sale Prices')
plt.xlabel('Actual Sale Price')
plt.ylabel('Predicted Sale Price')

residuals = []
for i in range(len(y_test)):
  residuals.append(y_test[i] - predictions[i])
plt.scatter(predictions, residuals)
plt.xlim(0,500000)
plt.ylim(-150000,150000)
plt.title('Residuals vs Predicted Values')
plt.xlabel('Predicted Sale Price')
plt.ylabel('Residuals')

"""# Section 2: Build a Convolutional Model
Using ```tensorflow```

### **Q2.1** Build the sequential model
"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (5, 5), activation='relu', input_shape=(28, 28, 1), padding='same', strides=(1,1)),
    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),
    tf.keras.layers.Conv2D(32, (5, 5), activation='relu', padding='same', strides=(1,1)),
    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='tanh'),
    tf.keras.layers.Dense(10)
])

"""### **Q2.2** Compile the model with the SGD optimizer"""

model.compile(optimizer=tf.keras.optimizers.SGD(0.005),
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.CategoricalAccuracy()])

"""### **Q2.3** Train the model with a batch size of 32 over 20 epochs"""

model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)

"""### **Q2.4** Evaluate the model and report the accuracy"""

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

"""### **Q2.5** Verify the model's weight matrices"""

weights = model.get_weights()
for weight in weights:
  print(np.shape(weight))